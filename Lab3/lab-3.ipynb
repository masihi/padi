{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 3: Partially observable Markov decision problems\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Modeling\n",
    "\n",
    "Consider once again the POMDP problem from the homework and represented in the transition diagram below.\n",
    "\n",
    "<img src=\"pomdp.png\" width=\"400px\">\n",
    "\n",
    "Recall that:\n",
    "\n",
    "* All transitions occur with probability 1 except those from state $A$, where the probabilities are indicated under the edge label.\n",
    "\n",
    "* At each step, the agent makes an observation corresponding to the letter in the state designation. Such observation occurs with probability 1.\n",
    "\n",
    "Consider throughout that $\\gamma=0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Implement your POMDP in Python. In particular,\n",
    "\n",
    "* Create a list with all the states;\n",
    "* Create a list with all the actions;\n",
    "* Create a list with all the observations\n",
    "* For each action, define a `numpy` array with the corresponding transition probabilities;\n",
    "* For each action, define a `numpy` array with the corresponding observation probabilities;\n",
    "* Define a `numpy` array with the cost describing the problem.\n",
    "\n",
    "The order for the states and actions used in the transition probability and cost matrices should match that in the lists of states and actions. \n",
    "\n",
    "**Note**: Don't forget to import `numpy`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = [ 'A', 'B1', 'B2', 'C', 'D', 'E', 'F']\n",
    "\n",
    "A = ['a', 'b', 'c']\n",
    "\n",
    "Z = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "# Preserving the order in X\n",
    "\n",
    "Pa =np.array([[0, 0.5, 0.5, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 0, 1, 0],\n",
    "     [0, 0, 0, 0, 0, 0, 1],\n",
    "     [0, 1, 0, 0, 0, 0, 0],\n",
    "     [0, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 0]\n",
    "    ])\n",
    "\n",
    "Pb =np.array([[0, 0.5, 0.5, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 0, 0, 1],\n",
    "     [0, 0, 0, 0, 0, 1, 0],\n",
    "     [0, 1, 0, 0, 0, 0, 0],\n",
    "     [0, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 0]\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "Pc =np.array([[0, 0.5, 0.5, 0, 0, 0, 0],\n",
    "     [0, 0, 0, 1, 0, 0, 0],\n",
    "     [0, 0, 0, 0, 1, 0, 0],\n",
    "     [0, 1, 0, 0, 0, 0, 0],\n",
    "     [0, 0, 1, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 0],\n",
    "     [1, 0, 0, 0, 0, 0, 0]\n",
    "    ])\n",
    "\n",
    "Za = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1]\n",
    "   ])\n",
    "\n",
    "Zb = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1]\n",
    "   ])\n",
    "\n",
    "Zc = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1]\n",
    "   ])\n",
    "\n",
    "C = np.array([\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [0, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sampling\n",
    "\n",
    "You are now going to sample random trajectories of your POMDP and observe the impact it has on the corresponding belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.\n",
    "\n",
    "Generate a random POMDP trajectory using a uniformly random policy. In particular, from a random initial state $x_0$ generate:\n",
    "\n",
    "1. A sequence of 10,000 states by selecting the actions uniformly at random;\n",
    "2. The corresponding sequence of 10,000 actions;\n",
    "3. The corresponding sequence of 10,000 observations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [2]\n",
      " [0]\n",
      " [2]]\n",
      "[[4]\n",
      " [2]\n",
      " [5]\n",
      " ...\n",
      " [3]\n",
      " [1]\n",
      " [3]]\n",
      "[[1]\n",
      " [4]\n",
      " [0]\n",
      " ...\n",
      " [2]\n",
      " [1]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "states = np.zeros( (10001, 1) , dtype = np.int8)\n",
    "actions = np.zeros( (10000, 1), dtype = np.int8)\n",
    "observations = np.zeros( (10000, 1), dtype = np.int8)\n",
    "\n",
    "Policy = [1.0/3.0, 1.0/3.0, 1.0/3.0]\n",
    "\n",
    "Probs = [Pa, Pb, Pc]\n",
    "\n",
    "Obs = [Za, Zb, Zc]\n",
    "\n",
    "\n",
    "states[0] = np.random.choice(7, p=[1.0/7.0,1.0/7.0,1.0/7.0,1.0/7.0,1.0/7.0,1.0/7.0,1.0/7.0])\n",
    "\n",
    "#print(states[0][0])\n",
    "#print(np.random.choice(3, p=policy))\n",
    "#print(np.random.choice(3, p=Obs[0][0]))\n",
    "\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    action = np.random.choice(3, p=Policy)\n",
    "    \n",
    "    actions[i] = action\n",
    "    states[i+1] = np.random.choice(7, p=Probs[action][states[i][0]])\n",
    "    observations[i] = np.random.choice(6, p=Obs[action][states[i+1][0]])\n",
    "\n",
    "'''\n",
    "for i in range(0,10):\n",
    "    print(states[i])\n",
    "for i in range(0,10):    \n",
    "    print(\"a\", actions[i])\n",
    "for i in range(0,10):\n",
    "    print(\"o\",observations[i])\n",
    "'''\n",
    "print(actions)\n",
    "print(states)\n",
    "print(observations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.\n",
    "\n",
    "For the POMDP trajectory generated in Activity 2, compute the corresponding sequence of beliefs, assuming that the agent does not know its initial state. Report the resulting beliefs, ignoring duplicate beliefs or beliefs whose distance is smaller than $10^{-3}$.\n",
    "\n",
    "**Note 1:** You may want to define a function `belief_update` that receives a belief, an action and an observation and returns the updated belief.\n",
    "\n",
    "**Note 2:** To compute the distance between vectors, you may find useful `numpy`'s function `linalg.norm`.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285], array([0. , 0.5, 0.5, 0. , 0. , 0. , 0. ]), array([0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1., 0.]), array([1., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 1., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0.])]\n",
      "10 beliefs\n"
     ]
    }
   ],
   "source": [
    "b0=[1/7,1/7,1/7,1/7,1/7,1/7,1/7]\n",
    "\n",
    "error = 1e-3\n",
    "\n",
    "beliefs = []\n",
    "beliefs.append(b0)\n",
    "\n",
    "def belief_update(belief, action, observation):\n",
    "    newBelief = np.matmul(np.matmul(belief, action), observation) \n",
    "   \n",
    "    \n",
    "    aux= sum(newBelief)\n",
    "    \n",
    "    if(aux!=0):\n",
    "        newBelief=newBelief/aux\n",
    "    return newBelief\n",
    "\n",
    "def check_new_belief(newb):\n",
    "    append = True\n",
    "    for b in beliefs:\n",
    "        if( np.linalg.norm(b - newb) <= error):\n",
    "            append = False\n",
    "    if(append == True):\n",
    "        beliefs.append(newb)\n",
    "\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    for b in beliefs:\n",
    "        newb = belief_update(b, \n",
    "                             Probs[ actions[i][0] ], \n",
    "                             np.diag( Obs[ actions[i][0] ][ :, observations[i][0] ] ))\n",
    "        check_new_belief(newb)\n",
    "           \n",
    "print(beliefs)\n",
    "print(len(beliefs), \"beliefs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solution methods\n",
    "\n",
    "In this section you are going to compare different non-exact solution methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4\n",
    "\n",
    "Compute the solution for the underlying MDP and report the corresponding optimal policy and optimal cost-to-go. \n",
    "\n",
    "** Note:** You may reuse code from previous labs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi = \n",
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.         1.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]]\n",
      "\n",
      "Cost-to-go =\n",
      "[7.01107011 6.67896679 6.67896679 7.01107011 7.01107011 7.3099631\n",
      " 6.3099631 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gama = 0.9\n",
    "pi = np.ones( (7,3) ) / 7\n",
    "Q = np.zeros( (7,3) )\n",
    "quit = False\n",
    "i = 0\n",
    "I = np.eye(7)\n",
    "\n",
    "\n",
    "ca = C[:,0,None]\n",
    "cb = C[:,1,None]\n",
    "cc = C[:,2,None]\n",
    "\n",
    "\n",
    "while not quit:\n",
    "    Cpi = np.diag(pi[:, 0]).dot(ca) + np.diag(pi[:, 1]).dot(cb) + np.diag(pi[:, 2]).dot(cc)\n",
    "    Ppi = np.diag(pi[:, 0]).dot(Pa) + np.diag(pi[:, 1]).dot(Pb) + np.diag(pi[:, 2]).dot(Pc)\n",
    "    J = np.linalg.inv(I - gama * Ppi).dot(Cpi)\n",
    "    \n",
    "    Qa = ca + gama * Pa.dot(J)\n",
    "    Qb = cb + gama * Pb.dot(J)\n",
    "    Qc = cc + gama * Pc.dot(J)\n",
    "    \n",
    "    Q[:, 0, None] = Qa\n",
    "    Q[:, 1, None] = Qb\n",
    "    Q[:, 2, None] = Qc\n",
    "    \n",
    "    pinew = np.zeros((7, 3))\n",
    "    pinew[:, 0, None] = np.isclose(Qa, np.min([Qa, Qb, Qc], axis = 0), atol=1e-8, rtol=1e-8).astype(int)\n",
    "    pinew[:, 1, None] = np.isclose(Qb, np.min([Qa, Qb, Qc], axis = 0), atol=1e-8, rtol=1e-8).astype(int)\n",
    "    pinew[:, 2, None] = np.isclose(Qc, np.min([Qa, Qb, Qc], axis = 0), atol=1e-8, rtol=1e-8).astype(int)\n",
    "    \n",
    "    pinew = pinew / np.sum(pinew, axis=1, keepdims=True)\n",
    "    \n",
    "    quit = (pi == pinew).all()\n",
    "    pi = pinew\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "#print(\"Q = \\n%s\\n\" % Q)\n",
    "#print(i)\n",
    "print(\"pi = \\n%s\\n\" % pi)\n",
    "\n",
    "Policy = pi\n",
    "\n",
    "Ppi = Policy[:, 0, None]*Pa + Policy[:, 1, None]*Pb + Policy[:, 2, None]*Pc\n",
    "Cpi = (Policy * C).sum(axis=1)\n",
    "\n",
    "#print(Pp)\n",
    "#print(Cp)\n",
    "\n",
    "\n",
    "Jpi = np.dot(np.linalg.inv(I - gama * Ppi), Cpi)\n",
    "\n",
    "print(\"Cost-to-go =\\n%s\\n\" % Jpi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5\n",
    "\n",
    "For each of the beliefs computed in Activity 3, compute the action prescribed by:\n",
    "\n",
    "* The MLS heuristic;\n",
    "* The AV heuristic;\n",
    "* The Q-MDP heuristic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.01107011 7.01107011 7.01107011]\n",
      " [7.57896679 6.67896679 7.3099631 ]\n",
      " [6.67896679 7.57896679 7.3099631 ]\n",
      " [7.01107011 7.01107011 7.01107011]\n",
      " [7.01107011 7.01107011 7.01107011]\n",
      " [7.3099631  7.3099631  7.3099631 ]\n",
      " [6.3099631  6.3099631  6.3099631 ]]\n",
      "i = 0\n",
      "MLS([0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]) = 0\n",
      "AV([0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]) = 0\n",
      "Q_MDP([0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285, 0.14285714285714285]) = 0\n",
      "i = 1\n",
      "MLS([0.  0.5 0.5 0.  0.  0.  0. ]) = 1\n",
      "AV([0.  0.5 0.5 0.  0.  0.  0. ]) = 0\n",
      "Q_MDP([0.  0.5 0.5 0.  0.  0.  0. ]) = 0\n",
      "i = 2\n",
      "MLS([0. 0. 0. 0. 0. 0. 0.]) = 0\n",
      "AV([0. 0. 0. 0. 0. 0. 0.]) = 0\n",
      "Q_MDP([0. 0. 0. 0. 0. 0. 0.]) = 0\n",
      "i = 3\n",
      "MLS([0. 0. 0. 0. 0. 1. 0.]) = 0\n",
      "AV([0. 0. 0. 0. 0. 1. 0.]) = 0\n",
      "Q_MDP([0. 0. 0. 0. 0. 1. 0.]) = 0\n",
      "i = 4\n",
      "MLS([1. 0. 0. 0. 0. 0. 0.]) = 0\n",
      "AV([1. 0. 0. 0. 0. 0. 0.]) = 0\n",
      "Q_MDP([1. 0. 0. 0. 0. 0. 0.]) = 0\n",
      "i = 5\n",
      "MLS([0. 0. 0. 0. 1. 0. 0.]) = 0\n",
      "AV([0. 0. 0. 0. 1. 0. 0.]) = 0\n",
      "Q_MDP([0. 0. 0. 0. 1. 0. 0.]) = 0\n",
      "i = 6\n",
      "MLS([0. 0. 1. 0. 0. 0. 0.]) = 0\n",
      "AV([0. 0. 1. 0. 0. 0. 0.]) = 0\n",
      "Q_MDP([0. 0. 1. 0. 0. 0. 0.]) = 0\n",
      "i = 7\n",
      "MLS([0. 0. 0. 0. 0. 0. 1.]) = 0\n",
      "AV([0. 0. 0. 0. 0. 0. 1.]) = 0\n",
      "Q_MDP([0. 0. 0. 0. 0. 0. 1.]) = 0\n",
      "i = 8\n",
      "MLS([0. 0. 0. 1. 0. 0. 0.]) = 0\n",
      "AV([0. 0. 0. 1. 0. 0. 0.]) = 0\n",
      "Q_MDP([0. 0. 0. 1. 0. 0. 0.]) = 0\n",
      "i = 9\n",
      "MLS([0. 1. 0. 0. 0. 0. 0.]) = 1\n",
      "AV([0. 1. 0. 0. 0. 0. 0.]) = 1\n",
      "Q_MDP([0. 1. 0. 0. 0. 0. 0.]) = 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "OptimalCost = Q\n",
    "OptimalPolicy = pi\n",
    "print(OptimalCost)\n",
    "\n",
    "mls = []\n",
    "av = []\n",
    "q_mdp = []\n",
    "\n",
    "def MLS(belief):\n",
    "    state = np.argmax(belief)\n",
    "    mls.append( np.argmax(OptimalPolicy[state]) )\n",
    "    return np.argmax(OptimalPolicy[state])\n",
    "\n",
    "def AV(belief):\n",
    "    av.append( np.argmax(np.dot(belief, OptimalPolicy)) )\n",
    "    return np.argmax(np.dot(belief, OptimalPolicy))\n",
    "    \n",
    "\n",
    "def Q_MDP(belief):\n",
    "    h = np.dot(belief, OptimalCost)\n",
    "    q_mdp.append( np.argmin(h) )\n",
    "    return np.argmin(h)\n",
    "    \n",
    "\n",
    "i = 0\n",
    "for b in beliefs:\n",
    "    print(\"i = %d\" % i)\n",
    "    print(\"MLS(%s) = %s\" % (b, MLS(b)))\n",
    "    print(\"AV(%s) = %s\" % (b, AV(b)))\n",
    "    print(\"Q_MDP(%s) = %s\" % (b, Q_MDP(b)))\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6\n",
    "\n",
    "Suppose that the optimal cost-to-go function for the POMDP can be represented using the $\\alpha$-vectors\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{bmatrix}\n",
    "8.39727208 \\\\\n",
    "8.70168739 \\\\\n",
    "7.80168739 \\\\\n",
    "8.39727208 \\\\\n",
    "8.39727208 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.70168739 \\\\\n",
    "7.80168739 \\\\\n",
    "8.02145332 \\\\\n",
    "8.83145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.70168739 \\\\\n",
    "7.80168739 \\\\\n",
    "8.83145332 \\\\\n",
    "8.02145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.39727208 \\\\\n",
    "7.80168739 \\\\\n",
    "8.70168739 \\\\\n",
    "8.39727208 \\\\\n",
    "8.39727208 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "7.80168739 \\\\\n",
    "8.70168739 \\\\\n",
    "8.02145332 \\\\\n",
    "8.83145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "7.80168739 \\\\\n",
    "8.70168739 \\\\\n",
    "8.83145332 \\\\\n",
    "8.02145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.39727208 \\\\\n",
    "8.2192667  \\\\\n",
    "8.2192667  \\\\\n",
    "8.39727208 \\\\\n",
    "8.39727208 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.2192667  \\\\\n",
    "8.2192667  \\\\\n",
    "8.02145332 \\\\\n",
    "8.83145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "8.42645332 \\\\\n",
    "8.2192667  \\\\\n",
    "8.2192667  \\\\\n",
    "8.83145332 \\\\\n",
    "8.02145332 \\\\\n",
    "8.55748144 \\\\\n",
    "7.55748144\n",
    "\\end{bmatrix},\\right\\}$$\n",
    "\n",
    "where the first 3 vectors correspond to action $a$, the middle three vectors correspond to action $b$ and the last 3 vectors correspond to action $c$. Using the $\\alpha$-vectors above, \n",
    "\n",
    "* Represent the the optimal cost-to-go function for all beliefs of the form $\\mathbf{b}=[0, \\epsilon, 1-\\epsilon, 0, 0, 0, 0]$, with $\\epsilon\\in[0,1]$. \n",
    "* Compare the optimal policy with the MDP heuristics from Activity 5 in the beliefs computed in Activity 3.\n",
    "\n",
    "** Note: ** Don't forget to import `matplotlib`, and use the magic `%matplotlib notebook`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De 0 a cerca de 0.5 que é a intersecção entre acção a e acção b escolhe-se a acção a. Entre cerca de 0.5 a 0.55, que é a intersecção entre acção b e acção c, deve-se escolher a acção b. De cerca de 0.55 a 1 deve-se escolher a acção c.\n",
      "i = 0\n",
      "MLS([[0.  0.5 0.5 0.  0.  0.  0. ]]) = 1\n",
      "AV([[0.  0.5 0.5 0.  0.  0.  0. ]]) = 0\n",
      "Q_MDP([[0.  0.5 0.5 0.  0.  0.  0. ]]) = 6\n",
      "Optimal Policy([[0.  0.5 0.5 0.  0.  0.  0. ]]) = 2\n",
      "i = 1\n",
      "MLS([[0. 1. 0. 0. 0. 0. 0.]]) = 1\n",
      "AV([[0. 1. 0. 0. 0. 0. 0.]]) = 1\n",
      "Q_MDP([[0. 1. 0. 0. 0. 0. 0.]]) = 3\n",
      "Optimal Policy([[0. 1. 0. 0. 0. 0. 0.]]) = 1\n",
      "i = 2\n",
      "MLS([[0. 0. 1. 0. 0. 0. 0.]]) = 0\n",
      "AV([[0. 0. 1. 0. 0. 0. 0.]]) = 0\n",
      "Q_MDP([[0. 0. 1. 0. 0. 0. 0.]]) = 0\n",
      "Optimal Policy([[0. 0. 1. 0. 0. 0. 0.]]) = 0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "b1 = np.array([[0. , 0.5, 0.5, 0. , 0. , 0. , 0. ]])\n",
    "b2 = np.array([[0., 1., 0., 0., 0., 0., 0.]])\n",
    "b3 = np.array([[0., 0., 1., 0., 0., 0., 0.]])\n",
    "\n",
    "\n",
    "OptimalCost = np.array([\n",
    "    [8.39727208, 8.42645332, 8.42645332, 8.39727208, 8.42645332, 8.42645332, 8.39727208, 8.42645332, 8.42645332 ],\n",
    "    [8.70168739, 8.70168739, 8.70168739, 7.80168739, 7.80168739, 7.80168739, 8.2192667, 8.2192667, 8.2192667],\n",
    "    [7.80168739, 7.80168739, 7.80168739, 8.70168739, 8.70168739, 8.70168739, 8.2192667, 8.2192667, 8.2192667],\n",
    "    [8.39727208, 8.02145332, 8.83145332, 8.39727208, 8.02145332, 8.83145332, 8.39727208, 8.02145332, 8.83145332],\n",
    "    [8.39727208, 8.83145332, 8.02145332, 8.39727208, 8.83145332, 8.02145332, 8.39727208, 8.83145332, 8.02145332],\n",
    "    [8.55748144, 8.55748144, 8.55748144, 8.55748144, 8.55748144, 8.55748144, 8.55748144, 8.55748144, 8.55748144],\n",
    "    [7.55748144, 7.55748144, 7.55748144, 7.55748144, 7.55748144, 7.55748144, 7.55748144, 7.55748144, 7.55748144]\n",
    "])\n",
    "\n",
    "alpha1 = [ 8.70168739, 7.80168739]\n",
    "alpha2 = [ 7.80168739, 8.70168739]\n",
    "alpha3 = [ 8.2192667, 8.2192667]\n",
    "\n",
    "alphas = [alpha1, alpha2, alpha3]\n",
    "\n",
    "plt.plot(alpha1)\n",
    "plt.plot(alpha2)\n",
    "plt.plot(alpha3)\n",
    "\n",
    "plt.title(\"Optimal Policy\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"De 0 a cerca de 0.5 que é a intersecção entre acção a e acção b escolhe-se a acção a. Entre cerca de 0.5 a 0.55, que é a intersecção entre acção b e acção c, deve-se escolher a acção b. De cerca de 0.55 a 1 deve-se escolher a acção c.\")\n",
    "\n",
    "def optimalCostToGo(belief):\n",
    "    J = np.dot(belief, OptimalCost)\n",
    "    index = np.argmin(J)\n",
    "    return index//3\n",
    "\n",
    "J1 = optimalCostToGo(b1)\n",
    "J2 = optimalCostToGo(b2)\n",
    "J3 = optimalCostToGo(b3)\n",
    "\n",
    "Jays = []\n",
    "Jays.append(J1)\n",
    "Jays.append(J2)\n",
    "Jays.append(J3)\n",
    "\n",
    "\n",
    "lastbeliefs = []\n",
    "lastbeliefs.append(b1)\n",
    "lastbeliefs.append(b2)\n",
    "lastbeliefs.append(b3)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for b in lastbeliefs:\n",
    "    print(\"i = %d\" % i)\n",
    "    print(\"MLS(%s) = %s\" % (b, MLS(b)))\n",
    "    print(\"AV(%s) = %s\" % (b, AV(b)))\n",
    "    print(\"Q_MDP(%s) = %s\" % (b, Q_MDP(b)))\n",
    "    print(\"Optimal Policy(%s) = %s\"%(b, Jays[i]))\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your comments here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
